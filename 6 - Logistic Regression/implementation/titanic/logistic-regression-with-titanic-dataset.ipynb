{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<img src=\"https://www.machinelearningplus.com/wp-content/uploads/2017/09/linear_vs_logistic_regression.jpg\" class=\"center\" width=\"600\" >\n<h4 align=\"right\">Source: Machine Learning Plus</h4>\n## <div style=\"text-align: center\" >Logistic Regression with Titanic Dataset</div>\n<div style=\"text-align: center\"> Being a part of Kaggle gives me unlimited access to learn, share and grow as a Data Scientist. In this kernel, I want to solve <font color=\"red\"><b>Titanic competition</b></font>, a popular machine learning dataset using <font color=\"red\"><b>Logistic Regression Classifier</b></font>. This kernel is a part of my machine learning series articles. If you would like to find out more about other machine learning models, please checkout this <a href=\"https://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic/edit/run/13339359\">this</a> kernel. </b> I will also describe how best to evaluate model results along with many other tips. So let's get started.</div>\n\n\n***\n<div style=\"text-align:center\"> If there are any recommendations/changes you would like to see in this notebook, please <b>leave a comment</b>. Any feedback/constructive criticism would be genuinely appreciated. <b>This notebook is always a work in progress. So, please stay tuned for more to come.</b></div>\n\n\n<div style=\"text-align:center\">If you like this notebook or find this notebook helpful, Please feel free to <font color=\"red\"><b>UPVOTE</b></font> and/or <font color=\"Blue\"><b>leave a comment.</b></font></div><br>\n\n<div style=\"text-align: center\"><b>You can also Fork and Run this kernel from <a href=\"https://github.com/masumrumi\">Github</b></a>\n    </div>\n\n### <div style=\"text-align: center\">Stay Tuned for More to Come!!</div>","metadata":{}},{"cell_type":"code","source":"# Import necessary modules for data analysis and data visualization. \n# Data analysis modules\n# Pandas is probably the most popular and important modules for any work related to data management. \nimport pandas as pd\n\n# numpy is a great library for doing mathmetical operations. \nimport numpy as np\n\n# Some visualization libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n## Importing the datasets\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n\n## Some other snippit of codes to get the setting right \n## This is so that the chart created by matplotlib can be shown in the jupyter notebook. \n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning\nimport os ## imporing os\nprint(os.listdir(\"../input/\")) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**> Sample train dataset**","metadata":{}},{"cell_type":"code","source":"## Take a look at the overview of the dataset. \ntrain.sample(5)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**> Sample test dataset**","metadata":{}},{"cell_type":"code","source":"test.sample(5)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am going to do all sorts of preparation ( including data munging, preparation, replacing NULL values, standard scaling...dummy variables) on the Titanic dataset to make it ready for the machine learning algorithm. If you would like to find out how I did it step-by-step. Please click <a href=\"https://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic\"> here.</a>","metadata":{}},{"cell_type":"code","source":"## saving passenger id in advance in order to submit later. \npassengerid = test.PassengerId\n\n## Replacing the null values in the Embarked column with the mode. \ntrain.Embarked.fillna(\"C\", inplace=True)\n\n## Concat train and test into a variable \"all_data\"\nsurvivers = train.Survived\n\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train,test], ignore_index=False)\n\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)\n\nall_data.Cabin = [i[0] for i in all_data.Cabin]\n\nwith_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]\n\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()\n\ndef cabin_estimator(i):\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n    \n\n##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers\n\nmissing_value = test[(test.Pclass == 3) & (test.Embarked == \"S\") & (test.Sex == \"male\")].Fare.mean()\n## replace the test.fare null values with test.fare mean\ntest.Fare.fillna(missing_value, inplace=True)\n\n## dropping the three outliers where Fare is over $500 \ntrain = train[train.Fare < 500]\n\n# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\n# Creating a new colomn with a \ntrain['name_length'] = [len(i) for i in train.Name]\ntest['name_length'] = [len(i) for i in test.Name]\n\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a\n\n\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\ntest['nLength_group'] = test['name_length'].map(name_length_group)\n\n## Here \"map\" is python's built-in function. \n## \"map\" function basically takes a function and \n## returns an iterable list/tuple or in this case series. \n## However,\"map\" can also be used like map(function) e.g. map(name_length_group) \n## or map(function, iterable{list, tuple}) e.g. map(name_length_group, train[feature]]). \n## However, here we don't need to use parameter(\"size\") for name_length_group because when we \n## used the map function like \".map\" with a series before dot, we are basically hinting that series \n## and the iterable. This is similar to .append approach in python. list.append(a) meaning applying append on list. \n\n## cuts the column by given bins based on the range of name_length\n#group_names = ['short', 'medium', 'good', 'long']\n#train['name_len_group'] = pd.cut(train['name_length'], bins = 4, labels=group_names)\n\n## Title\n## get the title from the name\ntrain[\"title\"] = [i.split('.')[0] for i in train.Name]\ntrain[\"title\"] = [i.split(',')[1] for i in train.title]\ntest[\"title\"] = [i.split('.')[0] for i in test.Name]\ntest[\"title\"]= [i.split(',')[1] for i in test.title]\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## train Data\ntrain[\"title\"] = [i.replace('Ms', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n\n\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## test data\ntest['title'] = [i.replace('Ms', 'Miss') for i in test.title]\ntest['title'] = [i.replace('Dr', 'rare') for i in test.title]\ntest['title'] = [i.replace('Col', 'rare') for i in test.title]\ntest['title'] = [i.replace('Dona', 'rare') for i in test.title]\ntest['title'] = [i.replace('Rev', 'rare') for i in test.title]\n\n## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1\n\ndef family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)\n\ntrain['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]\n\ntrain.drop(['Ticket'], axis=1, inplace=True)\n\ntest.drop(['Ticket'], axis=1, inplace=True)\n\n## Calculating fare based on family size. \ntrain['calculated_fare'] = train.Fare/train.family_size\ntest['calculated_fare'] = test.Fare/test.family_size\n\ndef fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\n\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 4, labels=groups)\n\ntrain.drop(['PassengerId'], axis=1, inplace=True)\n\ntest.drop(['PassengerId'], axis=1, inplace=True)\n\n\ntrain = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\ntest.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)\n\n## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \ntrain = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)\n\n## Importing RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \ndef completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df\n\n## Implementing the completing_age function in both train and test dataset. \ncompleting_age(train)\ncompleting_age(test);\n\n## create bins for age\ndef age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);\n\n\"\"\"train.drop('Age', axis=1, inplace=True)\ntest.drop('Age', axis=1, inplace=True)\"\"\"\n\n# separating our independent and dependent variable\nX = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]\n\n\n#age_filled_data_nor = NuclearNormMinimization().complete(df1)\n#Data_1 = pd.DataFrame(age_filled_data, columns = df1.columns)\n#pd.DataFrame(zip(Data[\"Age\"],Data_1[\"Age\"],df[\"Age\"]))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .33, random_state = 0)\n\n# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\ntrain_x = sc.fit_transform(X_train)\n## transforming \"test_x\"\ntest_x = sc.transform(X_test)\n\n## transforming \"The testset\"\ntest = sc.transform(test)\n\n## changing calculated_fare type\ntrain.calculated_fare = train.calculated_fare.astype(float)\n\n## Using StratifiedShuffleSplit\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n## Using standard scale for the whole dataset.\n\n## saving the feature names for decision tree display\ncolumn_names = X.columns\n\nX = sc.fit_transform(X)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Logistic Regression Classifier</h1>","metadata":{}},{"cell_type":"markdown","source":"**Logistic Regression**. Logistic regression is a famous classifier still used today frequently despite its age. It is a regression similar to **Linear regression**, yet operates as a classifier. To understand logistic regression, we should have some idea about linear regression. Let's have a look at it. \n\nHopefully, we all know that any linear equation can be written in the form of...\n\n# $$ {y} = mX + b $$\n\n* Here, m = slope of the regression line. it represents the relationship between X and y. \n* b = y-intercept. \n* x and y are the points location in x_axis and y_axis respectively. \n<br/>\n\nIf you want to know how, check out this [video](https://www.khanacademy.org/math/algebra/two-var-linear-equations/writing-slope-intercept-equations/v/graphs-using-slope-intercept-form). So, this slope equation can also be written as...\n\n## $$ y = \\beta_0 + \\beta_1 x + \\epsilon \\\\ $$\n\nThis is the equation for a simple linear regression.\nhere,\n* y = Dependent variable. \n* $\\beta_0$ = the intercept, it is constant. \n* $\\beta_1$ = Coefficient of independent variable. \n* $x$ = Indepentent variable. \n* $ \\epsilon$ = error or residual. \n\n\nWe use this function to predict the value of a dependent variable with the help of only one independent variable. Therefore this regression is called **Simple Linear Regression.** \n\nSimilar to **Simple Linear Regression**, there is **Multiple Linear Regression** which can be used to predict dependent variable using multiple independent variables. Let's look at the equation for **Multiple Linear Regression**, \n\n## $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n\n\nIf you would like to know more about **Linear Regression** checkout this [kernel](https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing). \n\nSo, we know/reviewed a bit about linear regression, and therefore we know how to deal with data that looks like this, \n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png\" width=\"600\">\n\nHere the data point's in this graph is continuous and therefore the problem is a regression one. However, what if we have data that when plotted in a scatter graph, looks like this...\n","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize = (12,10))\nplt.scatter(train.Age, train.Survived);\nplt.xlabel(\"Age\")\nplt.ylabel('Survival Status');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here the data points are not continuous; rather categorical. The two horizontal dot lines represent the survival status in the y-axis and age in the x-axis. This is probably not the best graph to explain logistic regression. For the convenience of understanding the model, let's look at a similar scatter plot with some characteristics.\n\n<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/39_blog_image_3.png\" width=\"600\">\n<h5 align=\"right\">SuperDataScience team</h5>\n\nThis chart clearly divides the binary categorical values in the x-axis, keeping most of the 0's on the left side, and 1's on the right side. So, now that the distinction is apparent, we can use our knowledge of linear regression and come up with a regression line. So, how can we apply a regression line to explain this data?\n\n<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/39_blog_image_4.png\" width=\"800\">\n<h5 align=\"right\">SuperDataScience team</h5>\n\nAs you can see from the chart above, The linear regression is probably not the best approach to take for categorical data. The Linear regression line barely aligns with the data points, and even if in some best-case scenario we were to use straight regression line, we would end up with a considerable error rate, which is super inconvenient. This is where logistic regression comes in. \n\n #### This part of the kernel is a working progress. Please check back again for future updates.####","metadata":{}},{"cell_type":"markdown","source":"Logistic Regression notes for StatQuest: \n\nLogistic regression does not have the concept of residual and therefore can't calculate sum of the squared residuals to fit a line with the data. Instead it fit a line with the help of something called \"maximum likelihood\". Let's describe more about this \n\nLogistic regression uses a \"S\" shaped line to fit the data. \n","metadata":{}},{"cell_type":"code","source":"# import LogisticRegression model in python. \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n## call on the model object\nlogreg = LogisticRegression(solver='liblinear')\n\n## fit the model with \"train_x\" and \"train_y\"\nlogreg.fit(X_train,y_train)\n\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \n## we use \"test_x\" portion of the data(this data was not used to fit the model) to predict model outcome. \ny_pred = logreg.predict(X_test)\n\n## Once predicted we save that outcome in \"y_pred\" variable.\n## Then we compare the predicted value( \"y_pred\") and actual value(\"test_y\") to see how well our model is performing. \n\nprint (\"So, Our accuracy Score is: {}\".format(round(accuracy_score(y_pred, y_test),4)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}